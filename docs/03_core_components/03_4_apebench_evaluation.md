[English](#english-version) | [中文](#chinese-version)
<a name="english-version"></a>

# 3.4 Evaluation Pipeline: Syntactic & Semantic Checks

This section describes the evaluation pipeline for APE-Bench I, which consists of two main stages: syntactic verification (checking if patches compile) and semantic judgment (checking if patches fulfill the instruction).

## Overview of the Two-Stage Evaluation (Paper Section 4)

A task is considered successfully completed by an LLM only if its generated patch passes *both* syntactic and semantic verification.

**Figure 6 from the paper illustrates this workflow:** Model-generated patch -> Syntactic Verification (Lean Compiler via Eleanstic) -> Semantic Judgement (LLM-as-a-Judge).

### 1. Syntactic Verification

*   **Purpose**: To ensure the model-generated patch results in Lean code that is grammatically correct and type-checks within the specific versioned environment of Mathlib4 from which the task originated.
*   **Mechanism**: 
    1.  The [`verification_manager.py`](../src/apebench/evaluation_pipelines/verification_manager.py) orchestrates the verification of model-generated patches.
    2.  It calls `src.apebench.evaluation_pipelines.gather_results --pipeline patch` to collect and prepare patch data from multiple generation output files. This process creates a flat JSONL file where each line contains `commit_hash`, `code` (the patched Lean code), and other metadata.
    3.  It then invokes **Eleanstic** (`src.eleanstic.main verify`) with this prepared JSONL file, specifying `--commit_id_key "commit_hash"` and `--code_key "code"`.
    4.  For each patch, Eleanstic:
        *   Restores the correct Mathlib build environment for the specified commit.
        *   Writes the code to a temporary file.
        *   Invokes `lake env lean <temp_file.lean>` on the file.
        *   Captures the verification result (success/failure).
    5.  Eleanstic writes the verification results to JSONL files in the specified output directory.
    6.  `verification_manager.py` then calls `gather_results --pipeline verification` to collect all these results into a single file.
    7.  Finally, it merges verification results with the original generation data and calculates syntactic verification metrics.
*   **Relevant Code**: 
    *   `src/apebench/evaluation_pipelines/verification_manager.py` - Manages the entire verification workflow.
    *   `src/apebench/evaluation_pipelines/gather_results.py` - Prepares data for Eleanstic and collects results.
    *   `src/eleanstic/` - Backend service for Lean verification.

### 2. Semantic Judgement (LLM-as-a-Judge)

*   **Purpose**: Syntactic correctness alone is not sufficient. The patch must also fulfill the *intent* described in the natural language `Instruction`.
*   **Mechanism (Paper Section 4.2)**:
    1.  The [`evaluation_manager.py`](../src/apebench/evaluation_pipelines/evaluation_manager.py) orchestrates the semantic evaluation.
    2.  It starts by filtering patches that passed syntactic verification through the `filter_verified_data` function, then flattens the data structure with `flatten_results` to prepare for judgment.
    3.  It invokes `src.apebench.inference.run_inference --pipeline judgement` to run the LLM-as-Judge evaluation on these syntactically valid patches with parameters including:
        ```bash
        python -m src.apebench.inference.run_inference
            --pipeline judgement
            --input_file /path/to/flattened_results.jsonl
            --output_file /path/to/judgement_output.jsonl 
            --model_name aws_sdk_claude37_sonnet@thinking
            --temperature 0.0
            --n_responses 4
            --max_workers 8
        ```
    4.  The judge LLM (e.g., Claude Sonnet 3.7 in thinking mode as specified in the paper) is prompted with:
        *   The original task instruction.
        *   The pre-file (code before the edit).
        *   The candidate (syntactically valid) patch generated by the model.
    5.  The judge evaluates the patch across multiple dimensions including correctness, completeness, and adherence to requirements.
    6.  Results are collected via `gather_results.py --pipeline judgement`.
    7.  **Voting Strategy**: To improve robustness, a `sample@4` strategy is used (as configured by `config.judgement.n_responses`). Four independent judgments are sampled from the judge LLM, and a majority vote is calculated by counting positive ("good", "excellent") vs. negative ("poor", "unacceptable") assessments for each dimension of the evaluation.
*   **Relevant Code**: 
    *   `src/apebench/evaluation_pipelines/evaluation_manager.py` - Manages the semantic judgment workflow.
    *   `src/apebench/inference/run_inference.py` - Contains the judgement pipeline implementation.
    *   `src/apebench/evaluation_pipelines/gather_results.py` - For result collection and filtering.

## Metrics

The primary metric reported in the paper (Section 5.1) is **pass@k**.
*   For each task, $n$ candidate patches are generated (e.g., $n=20$).
*   `pass@k` estimates the probability that at least one of $k$ candidates (e.g., $k=16$) passes the evaluation criteria.
*   The implementation for this calculation is found in `src/apebench/evaluation_pipelines/gather_results.py` in the `pass_at_k` function:
    ```python
    def pass_at_k(n, c, k):
        """
        Calculate pass@k metric
        
        Args:
            n: Total number of generations
            c: Number of generations that pass verification/evaluation
            k: k value to evaluate
        
        Returns:
            pass@k metric value
        """
        if n == 0:
            return 0.0
        if n - c < k:
            return 1.0
        return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))
    ```
*   The paper reports:
    *   **Lean Compile pass@16**: Syntactic success rate (calculated by `verification_manager.py`).
    *   **LLM-Judge pass@16**: Semantic success rate (conditional on syntactic success, calculated by `evaluation_manager.py`).
    *   **Semantic Drop**: The percentage decrease from Lean Compile success to LLM-Judge success, indicating how often syntactically valid patches fail semantically.

The metrics calculations are coordinated by `extract_verification_data` and `calculate_metrics` functions in the APE-Bench utilities.

## Error Analysis (Paper Section 5.4)

The paper also presents a taxonomy of semantic errors (Table 4), such as "Component Omission," "Incompleteness Issue," etc. This suggests that the semantic evaluation (LLM-as-a-Judge) might also provide or be analyzable for more fine-grained failure reasons, although the primary output is pass/fail.

## Usage

*   The evaluation pipeline is typically executed as a two-step process:
    1.  **Syntactic Verification**: 
        ```bash
        # After patch generation is complete:
        python -m src.apebench.evaluation_pipelines.verification_manager --config path/to/config.yaml
        ```
        This processes all generated patches through Eleanstic and produces detailed verification results.
        
    2.  **Semantic Judgement**: 
        ```bash
        # After syntactic verification is complete:
        python -m src.apebench.evaluation_pipelines.evaluation_manager --config path/to/config.yaml
        ```
        This applies LLM-as-Judge evaluation to syntactically valid patches and computes final metrics.

*   Both stages store detailed results and metrics in the specified output directories. The `k_ratio` parameter in the config controls the k/n ratio for pass@k calculations (default is 0.8, so for n=20, k=16).

---

Next: [Scripts and Configuration](./03_5_apebench_scripts_config.md) 

<a name="chinese-version"></a>

## 中文翻译 (Chinese Translation)

# 3.4 评估流程：语法与语义检查

本节描述 APE-Bench I 的评估流程，它由两个主要阶段组成：语法验证（检查补丁是否编译）和语义判断（检查补丁是否完成指令）。

## 两阶段评估概述（论文第 4 节）

只有当 LLM 生成的补丁同时通过语法和语义验证时，该任务才被视为成功完成。

**论文中的图 6阐释了此工作流程：** 模型生成的补丁 -> 语法验证（通过 Eleanstic 的 Lean 编译器）-> 语义判断（作为裁判的 LLM）。

### 1. 语法验证

*   **目的**：确保模型生成的补丁能产生在任务来源的特定 Mathlib4 版本环境中语法正确且类型检查通过的 Lean 代码。
*   **机制**：
    1.  [`verification_manager.py`](../src/apebench/evaluation_pipelines/verification_manager.py) 负责协调模型生成补丁的验证过程。
    2.  它调用 `src.apebench.evaluation_pipelines.gather_results --pipeline patch` 来收集和准备来自多个生成输出文件的补丁数据。此过程会创建一个扁平化的 JSONL 文件，其中每行包含 `commit_hash`、`code`（修补后的 Lean 代码）和其他元数据。
    3.  然后，它使用这个准备好的 JSONL 文件调用 **Eleanstic** (`src.eleanstic.main verify`)，并指定 `--commit_id_key "commit_hash"` 和 `--code_key "code"`。
    4.  对于每个补丁，Eleanstic：
        *   恢复指定提交的正确 Mathlib 构建环境。
        *   将代码写入临时文件。
        *   在该文件上调用 `lake env lean <temp_file.lean>`。
        *   捕获验证结果（成功/失败）。
    5.  Eleanstic 将验证结果写入指定输出目录中的 JSONL 文件。
    6.  然后 `verification_manager.py` 调用 `gather_results --pipeline verification` 将所有这些结果收集到一个文件中。
    7.  最后，它将验证结果与原始生成数据合并，并计算语法验证指标。
*   **相关代码**：
    *   `src/apebench/evaluation_pipelines/verification_manager.py` - 管理整个验证工作流程。
    *   `src/apebench/evaluation_pipelines/gather_results.py` - 为 Eleanstic 准备数据并收集结果。
    *   `src/eleanstic/` - Lean 验证的后端服务。

### 2. 语义判断（作为裁判的 LLM）

*   **目的**：仅有语法正确性是不够的。补丁还必须满足自然语言 `Instruction` 中描述的*意图*。
*   **机制（论文第 4.2 节）**：
    1.  [`evaluation_manager.py`](../src/apebench/evaluation_pipelines/evaluation_manager.py) 负责协调语义评估。
    2.  它首先通过 `filter_verified_data` 函数筛选通过语法验证的补丁，然后使用 `flatten_results` 扁平化数据结构以准备判断。
    3.  它调用 `src.apebench.inference.run_inference --pipeline judgement` 对这些语法有效的补丁运行作为裁判的 LLM 评估，参数包括：
        ```bash
        python -m src.apebench.inference.run_inference
            --pipeline judgement
            --input_file /path/to/flattened_results.jsonl
            --output_file /path/to/judgement_output.jsonl 
            --model_name aws_sdk_claude37_sonnet@thinking
            --temperature 0.0
            --n_responses 4
            --max_workers 8
        ```
    4.  裁判 LLM（例如论文中指定的处于思考模式的 Claude Sonnet 3.7）会被提示以下内容：
        *   原始任务指令。
        *   修改前文件（编辑前的代码）。
        *   模型生成的候选（语法有效的）补丁。
    5.  裁判评估补丁的多个维度，包括正确性、完整性以及是否符合要求。
    6.  通过 `gather_results.py --pipeline judgement` 收集结果。
    7.  **投票策略**：为提高鲁棒性，使用 `sample@4` 策略（由 `config.judgement.n_responses` 配置）。从裁判 LLM 中采样四个独立的判断，通过计算每个评估维度的正面评价（"good"、"excellent"）与负面评价（"poor"、"unacceptable"）的数量，确定多数票。
*   **相关代码**：
    *   `src/apebench/evaluation_pipelines/evaluation_manager.py` - 管理语义判断工作流程。
    *   `src/apebench/inference/run_inference.py` - 包含判断流程的实现。
    *   `src/apebench/evaluation_pipelines/gather_results.py` - 用于结果收集和筛选。

## 指标

论文（第 5.1 节）中报告的主要指标是 **pass@k**。
*   对于每个任务，生成 $n$ 个候选补丁（例如，$n=20$）。
*   `pass@k` 估计 $k$ 个候选补丁中（例如，$k=16$）至少有一个通过评估标准的概率。
*   此计算的实现在 `src/apebench/evaluation_pipelines/gather_results.py` 的 `pass_at_k` 函数中找到：
    ```python
    def pass_at_k(n, c, k):
        """
        计算 pass@k 指标
        
        参数:
            n: 生成总数
            c: 通过验证/评估的生成数量
            k: 要评估的 k 值
        
        返回:
            pass@k 指标值
        """
        if n == 0:
            return 0.0
        if n - c < k:
            return 1.0
        return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))
    ```
*   论文报告：
    *   **Lean Compile pass@16**：语法成功率（由 `verification_manager.py` 计算）。
    *   **LLM-Judge pass@16**：语义成功率（以语法成功为条件，由 `evaluation_manager.py` 计算）。
    *   **Semantic Drop (语义下降)**：从 Lean 编译成功到 LLM 裁判成功的百分比下降，表明语法有效的补丁在语义上失败的频率。

指标计算由 APE-Bench 工具中的 `extract_verification_data` 和 `calculate_metrics` 函数协调。

## 错误分析（论文第 5.4 节）

论文还提出了语义错误的分类法（表 4），例如"组件遗漏"、"不完整性问题"等。这表明语义评估（作为裁判的 LLM）也可能提供或可用于分析更细粒度的失败原因，尽管主要输出是成功/失败。

## 用法

*   评估流程通常分两步执行：
    1.  **语法验证**：
        ```bash
        # 补丁生成完成后：
        python -m src.apebench.evaluation_pipelines.verification_manager --config path/to/config.yaml
        ```
        这将通过 Eleanstic 处理所有生成的补丁，并产生详细的验证结果。

    2.  **语义判断**：
        ```bash
        # 语法验证完成后：
        python -m src.apebench.evaluation_pipelines.evaluation_manager --config path/to/config.yaml
        ```
        这将对语法有效的补丁应用作为裁判的 LLM 评估，并计算最终指标。

*   两个阶段都会将详细结果和指标存储在指定的输出目录中。配置中的 `k_ratio` 参数控制 pass@k 计算的 k/n 比率（默认为 0.8，因此对于 n=20，k=16）。

---

下一节: [脚本与配置](./03_5_apebench_scripts_config.md) 